---
title: "Capstone Milestone Report"
author: "Jeremiah Lowhorn"
date: "Monday, December 28, 2015"
output: html_document
---

#Synopsis
This milestone report is for the Coursera Data Science Capstone project. The purpose of the overall project is to construct an application that predicts the next word in a user defined sentence. We are provided with a text corpus that we are to perform exploratory analysis upon and use in model building for our word prediction algorithm. 

The text contains special characters, unneeded spaces, and profanity that must first be removed. The text comes from a document of Tweets, news articles, and blog posts. R's base package functions and regular expressions are used in this analysis to clean the data and make it ready to split into n-gram models. N-grams are parsed out groups of words in sentences with n items from the sentence. For example the sentence "How are you today?" could be split into the bigrams "How are" or "are you‚Äù. These n-grams will be used to predict the next word in the sentence based upon the user's input. 

Note: Due to system memory limits only 10,000 lines from each file will be read into this analysis. 

The data is available at the below link. The files used in the analysis are entitled *en_US.news.txt*, *en_US.blogs.txt*, and *en_US.twitter.txt*.
 
[Capstone Data](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

```{r,echo=FALSE,message=FALSE}
library(ngram)
library(dplyr)
library(ggplot2)
news <- readLines("C:/Users/Jeremiah Lowhorn/Desktop/final/en_US/en_US.news.txt", 10000,encoding="UTF-8")
blogs <- readLines("C:/Users/Jeremiah Lowhorn/Desktop/final/en_US/en_US.blogs.txt", 10000,encoding="UTF-8")
twitter <- readLines("C:/Users/Jeremiah Lowhorn/Desktop/final/en_US/en_US.twitter.txt", 10000,encoding="UTF-8")
profanity <- read.csv("C:/Users/Jeremiah Lowhorn/Desktop/final/en_US/profanity.txt", header=FALSE, stringsAsFactors=FALSE)

text <- c(news,twitter,blogs)

rm(news);rm(blogs);rm(twitter)

profanity <- profanity$V1

text <- gsub(pattern=';|\\.|!|\\?', x=text, replacement='ootoo')
text <- gsub(pattern="[^[:alpha:]]", x=text, replacement = ' ')
text <- tolower(text)
text <- gsub(pattern="\\W*\\b\\w{1,2}\\b", x=text, replacement=' ')
  
text <- gsub(pattern="\\s+", x=text, replacement=' ')

text <- unlist(strsplit(x=text, split='ootoo',fixed = TRUE))

text <- gsub(pattern=';|\\.|!|\\?', x=text, replacement='ootoo')

text <- gsub(pattern=paste0('\\<',profanity,collapse="|"),
                    x=text,
                   replacement="expletive ")

Trim <- function( x ) {
  gsub("(^[[:space:]]+|[[:space:]]+$)", "", x)
}

Make_Ngrams <- function(sentence_splits, size=2) {
  ngrams <- c()
  for (sentence in sentence_splits) {
    sentence <- Trim(sentence)
    if ((nchar(sentence) > 0) && (sapply(gregexpr("\\W+", sentence), length) >= size)) {
      ngs <- ngram(sentence , n=size)
      ngrams <- c(ngrams, get.ngrams(ngs))
    }
  }
  return (ngrams)
}

n1 <- Make_Ngrams(text,size=1)
n2 <- Make_Ngrams(text,size=2)
n3 <- Make_Ngrams(text,size=3)
n4 <- Make_Ngrams(text,size=4)

n1_df <- table(n1)
n1_df <- as.data.frame(n1_df) %>%
  arrange(desc(Freq))
n1_df$n1 <- as.character(n1_df$n1)

n2_df <- table(n2)
n2_df<- as.data.frame(n2_df) %>%
  arrange(desc(Freq))

n3_df <- table(n3)
n3_df <- as.data.frame(n3_df) %>%
  arrange(desc(Freq))

n4_df <- table(n4)
n4_df <- as.data.frame(n4_df) %>%
  arrange(desc(Freq))

library(ggplot2)

n1_df <- n1_df[1:10,]
Uniplot <- ggplot(n1_df,aes(reorder(n1,Freq),Freq)) +
  geom_bar(stat="identity") + 
  ggtitle("Top Ten Unigrams") + 
  coord_flip() +
  xlab("Unigrams") + 
  ylab("Frequency") + 
  theme_bw()

n2_df <- n2_df[1:10,]
Biplot <- ggplot(n2_df,aes(reorder(n2,Freq),Freq)) +
  geom_bar(stat="identity") + 
  ggtitle("Top Ten Bigrams") + 
  coord_flip() + 
  xlab("Bigrams") + 
  ylab("Frequency") +
  theme_bw()

n3_df <- n3_df[1:10,]
Triplot <- ggplot(n3_df,aes(reorder(n3,Freq),Freq)) +
  geom_bar(stat="identity") + 
  ggtitle("Top Ten Trigrams") + 
  coord_flip() + 
  xlab("Trigrams") + 
  ylab("Frequency") +
  theme_bw()

n4_df <- n4_df[1:10,]
Quadplot <- ggplot(n4_df,aes(reorder(n4,Freq),Freq)) +
  geom_bar(stat="identity") + 
  ggtitle("Top Ten Quartgrams") + 
  coord_flip() + 
  xlab("Quartgrams") + 
  ylab("Frequency") +
  theme_bw()

library(R.utils)

NewsSize <-file.info("C:/Users/193344/Desktop/final/en_US/en_US.news.txt")$size/(1024*1024)
BlogsSize <- file.info("C:/Users/193344/Desktop/final/en_US/en_US.blogs.txt")$size/(1024*1024)
TwitterSize <- file.info("C:/Users/193344/Desktop/final/en_US/en_US.twitter.txt")$size/(1024*1024)

NewsLines <- countLines("C:/Users/193344/Desktop/final/en_US/en_US.news.txt")
BlogsLines <- countLines("C:/Users/193344/Desktop/final/en_US/en_US.blogs.txt")
TwitterLines <- countLines("C:/Users/193344/Desktop/final/en_US/en_US.twitter.txt")

File <- c("News","Blogs","Twitter")
Size <- c(NewsSize,BlogsSize,TwitterSize)
Lines <- c(NewsLines,BlogsLines,TwitterLines)

Data <- data.frame(cbind(File,Size,Lines))

```


#File Statistics
```{r, echo=FALSE}
Data
```

#Exploratory Analysis
As seen from the below plots and tables the most common unigrams are stop words and for the most part the bi, tri, and quartgrams contain these same English stop words. Stop words are words such as "the", "that", and "it". Rather than trying to predict these words in the model they may need to be removed in order to make the model more accurate. Additionally the current profanity filter is set to replace the profane words to "expletive" using the base R function gsub().

###Number of Words in the Sample Data
```{r}
length(n1)
```

###Number of Profane Words Removed from the text
```{r}
length(n1[n1 %in% "expletive"])
```

###Unigram Table
```{r,echo=FALSE}
n1_df
```

###Unigram Plot
```{r, echo=FALSE}
Uniplot
```

```{r,echo=FALSE}
n2_df
```

###Bigram Analysis
```{r, echo=FALSE}
Biplot
```

###Trigram Table
```{r,echo=FALSE}
n3_df
```

###Trigram Analysis
```{r, echo=FALSE}
Triplot
```

###Quartgram Table
```{r,echo=FALSE}
n4_df
```

###Quartgram Analysis
```{r, echo=FALSE}
Quadplot
```

#Modeling & Next Steps
My current working model takes the sampled corpus as a character vector and finds all of the matches of the previous 1, 2, 3, or 4 words. It then searches the vector for all possible matches and returns the next possible word based upon frequency in the vector. This approach is extraordinarily inefficient and does not produce the most accurate results. Additionally cleaning takes longer than desired with the regex I am using. I need to explore the TM package and others in order to find a more efficient way to clean the text and remove profanity words. 

My idea(s) for the new model are as follows:

* Rather than using the current method to predict words I will attempt to create data tables for quartgrams, trigrams, and bigrams. 
* I will use R's sub setting power to find the first exact match of the phrase and return the  associated word for the sentence starting in the quartgram table. 
* If there is no match, or the length of the sentence isn't long enough the algorithm will work its way down to the bigram table. 
* If no word can be matched the most commonly used unigram word will be used, most likely "the".
